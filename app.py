
import os
import re
import time
import requests
import unicodedata
import streamlit as st
from rapidfuzz import fuzz, process
from openai import OpenAI

# ========= CONFIG =========
DEFAULT_SAFE_TARGET = 10          # poÄet "bezpeÄnÃ½ch" nÃ¡vrhÅ¯ ve vÃ½stupu
ARES_COUNT = 60                   # kolik zÃ¡znamÅ¯ tahat z ARES pro porovnÃ¡nÃ­
UA = "cz-name-checker/1.3 (+contact: owner@example.com)"

# ========= API KEY =========
def _get_api_key():
    key = os.getenv("OPENAI_API_KEY")
    if key:
        return key
    try:
        return st.secrets["OPENAI_API_KEY"]
    except Exception:
        return None

_api_key = _get_api_key()
if not _api_key:
    st.error("ChybÃ­ OPENAI_API_KEY. Nastav v Railway/Variables nebo ve Streamlit Secrets.")
    st.stop()
client = OpenAI(api_key=_api_key)

# ========= NORMALIZATION =========
LEGAL_SUFFIXES = [
    "s.r.o.", "sro", "a.s.", "as", "k.s.", "ks", "v.o.s.", "vos", "spol. s r.o.", "spol s r o"
]
GENERIC_WORDS = [
    "cz","czech","praha","brno","plzen","ostrava","group","holding","solutions","consulting",
    "system","systems","technology","technologies","services","service","studio","company","co",
    "global","international","advisory","adviser","advisers","media","marketing","agency","digital"
]

def strip_diacritics(s: str) -> str:
    norm = unicodedata.normalize("NFD", s or "")
    return "".join(ch for ch in norm if unicodedata.category(ch) != "Mn")

def norm_core(name: str) -> str:
    if not name:
        return ""
    s = name.lower().strip()
    s = strip_diacritics(s)
    for suf in LEGAL_SUFFIXES:
        s = s.replace(suf, " ")
    s = re.sub(r"[^0-9a-zA-Z\s]", " ", s)
    s = " ".join(s.split())
    parts = [p for p in s.split() if p not in GENERIC_WORDS]
    return " ".join(parts)

# ========= ARES SEARCH (v2 REST, zdroj OR) s retry =========
def ares_query(obchodni_jmeno: str, count: int = ARES_COUNT):
    base = "https://ares.gov.cz/ekonomicke-subjekty-v2/ekonomicke-subjekty"
    params = {
        "obchodniJmeno": obchodni_jmeno,
        "pocet": str(count),
        "razeni": "obchodniJmeno@asc",
        "zdroj": "OR"
    }
    headers = {"Accept": "application/json", "User-Agent": UA}

    last_err = None
    for attempt in range(3):
        try:
            r = requests.get(base, params=params, headers=headers, timeout=20)
            r.raise_for_status()
            data = r.json()
            items = data.get("ekonomickeSubjekty", []) or data.get("vysledky", [])
            out = []
            for it in items:
                of = it.get("obchodniJmeno") or it.get("obchodniJmenoText")
                ico = it.get("ico")
                if of:
                    out.append({"name": of, "ico": ico})
            return out
        except Exception as e:
            last_err = e
            time.sleep(1.2 * (2 ** attempt))
    st.warning(f"NepodaÅ™ilo se pÅ™ipojit k ARES (REST): {last_err}")
    return []

def ares_search_robust(candidate: str):
    """Zkus vÃ­ce variant dotazu: plnÃ½ nÃ¡zev, bez diakritiky, 1â€“2 slova jÃ¡dra."""
    queries = []
    full = (candidate or "").strip()
    queries.append(full)
    queries.append(strip_diacritics(full))

    core = norm_core(candidate)
    core_words = core.split()
    if len(core_words) >= 1:
        queries.append(core_words[0])
    if len(core_words) >= 2:
        queries.append(" ".join(core_words[:2]))

    seen = set()
    hits = []
    for q in queries:
        q = q.strip()
        if not q or q.lower() in seen:
            continue
        seen.add(q.lower())
        try:
            hits += ares_query(q, ARES_COUNT)
        except Exception:
            pass
        # deduplikace podle normalizovanÃ©ho jÃ¡dra
        dedup = {}
        for h in hits:
            key = norm_core(h["name"])
            if key and key not in dedup:
                dedup[key] = h
        hits = list(dedup.values())
    return hits

# ========= SIMILARITY =========
def max_similarity(candidate: str, corpus: list):
    """Vezmeme maximum ze 4 rÅ¯znÃ½ch metrik (opatrnÄ›jÅ¡Ã­)."""
    names = [h["name"] for h in corpus]
    if not names:
        return 0, None
    scorers = [fuzz.token_set_ratio, fuzz.token_sort_ratio, fuzz.partial_ratio, fuzz.QRatio]
    max_s = 0
    best_match = None
    for sc in scorers:
        match, score, idx = process.extractOne(candidate, names, scorer=sc)
        if score > max_s:
            max_s, best_match = score, match
    return max_s, best_match

# ========= NAME GENERATION =========
def generate_ai_names(keywords, style, n=10):
    prompt = (
        f"Vymysli {n} originÃ¡lnÃ­ch nÃ¡zvÅ¯ firmy, kterÃ© NEJSOU bÄ›Å¾nÃ½mi ÄeskÃ½mi slovy a nejsou generickÃ©.\n"
        f"KlÃ­ÄovÃ¡ slova/obor: {keywords}\n"
        f"Styl: {style or 'modernÃ­, struÄnÃ©, snadno vyslovitelnÃ©'}\n"
        f"PodmÃ­nky: 1 slovo, bez diakritiky, bez prÃ¡vnÃ­ch pÅ™Ã­pon, bez slov jako group/consulting/solutions.\n"
        f"PiÅ¡ jen nÃ¡zvy, kaÅ¾dÃ½ na novÃ½ Å™Ã¡dek."
    )
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.9,
    )
    text = resp.choices[0].message.content.strip()
    names = [line.strip("-â€¢ ").strip() for line in text.split("\n") if line.strip()]
    # zÃ¡kladnÃ­ filtrovÃ¡nÃ­
    clean = []
    for nm in names:
        core = norm_core(nm)
        if not core:
            continue
        if any(g in core.split() for g in GENERIC_WORDS):
            continue
        if not (4 <= len(core) <= 14):
            continue
        clean.append(nm[:70])
    # dedup
    seen, uniq = set(), []
    for nm in clean:
        key = norm_core(nm)
        if key not in seen:
            seen.add(key)
            uniq.append(nm)
    return uniq[:n]

def generate_safe_free_names(keywords, style, desired, free_threshold):
    """Vygeneruje kandidÃ¡ty a propustÃ­ jen ty, kterÃ© jsou pod zadanÃ½m prahem podobnosti."""
    results = []
    attempts = 0
    # maximÃ¡lnÄ› 6 iteracÃ­ nenÃ¡silnÄ›, aÅ¥ zbyteÄnÄ› netrÃ¡pÃ­me API
    while len(results) < desired and attempts < 6:
        attempts += 1
        candidates = generate_ai_names(keywords, style, desired)
        for cand in candidates:
            hits = ares_search_robust(cand)
            score, match = max_similarity(cand, hits)
            if score < free_threshold:
                results.append({
                    "NÃ¡vrh": cand,
                    "VÃ½sledek": "âœ… VolnÃ© (bez blÃ­zkÃ½ch shod)",
                    "NejbliÅ¾Å¡Ã­ shoda": match,
                    "SkÃ³re": round(score, 1)
                })
            if len(results) >= desired:
                break
    return results

# ========= UI =========
st.set_page_config(page_title="GenerÃ¡tor nÃ¡zvÅ¯ + ARES kontrola", layout="centered")
st.title("ğŸ§­ GenerÃ¡tor nÃ¡zvÅ¯ firem + kontrola v ARES (CZ) â€“ SAFE reÅ¾im")

with st.expander("âš™ï¸ NastavenÃ­"):
    mode = st.selectbox("ReÅ¾im", ["BezpeÄnÃ© nÃ¡zvy (doporuÄeno)", "KreativnÃ­ + kontrola (standard)"])
    n = st.slider("PoÄet vÃ½sledkÅ¯", 5, 30, 10)
    style = st.text_input("Styl (volitelnÃ©)", "modernÃ­, struÄnÃ©, nezamÄ›nitelnÃ©")
    free_thr = st.slider("PrÃ¡h pro 'VolnÃ©' (max. podobnost %)", 55, 85, 70)
    st.caption("ÄŒÃ­m niÅ¾Å¡Ã­ prÃ¡h, tÃ­m pÅ™Ã­snÄ›jÅ¡Ã­ filtr (mÃ©nÄ› nÃ¡vrhÅ¯ projde). DoporuÄeno 65â€“75 %.")

keywords = st.text_input("Zadej obor/klÃ­ÄovÃ¡ slova (napÅ™. prÃ¡vnÃ­ sluÅ¾by, AI Å¡kolenÃ­, kavÃ¡rna):")

if st.button("Vygenerovat a zkontrolovat"):
    if not keywords.strip():
        st.warning("Zadej aspoÅˆ obor nebo klÃ­ÄovÃ¡ slova.")
        st.stop()

    if mode == "BezpeÄnÃ© nÃ¡zvy (doporuÄeno)":
        with st.spinner("ğŸ”’ Generuji bezpeÄnÃ© nÃ¡zvy a pÅ™Ã­snÄ› ovÄ›Å™uji v ARESâ€¦"):
            results = generate_safe_free_names(keywords, style, desired=n, free_threshold=free_thr)
        if not results:
            st.error("NepodaÅ™ilo se najÃ­t dostateÄnÃ½ poÄet 'bezpeÄnÃ½ch' nÃ¡zvÅ¯. Zkus snÃ­Å¾it prÃ¡h nebo upÅ™esnit klÃ­ÄovÃ¡ slova.")
        else:
            st.success("Hotovo. Zde jsou vÃ½sledky:")
            st.dataframe(results, use_container_width=True)
    else:
        with st.spinner("ğŸ¨ Generuji kreativnÃ­ nÃ¡zvy a ovÄ›Å™uji v ARESâ€¦"):
            candidates = generate_ai_names(keywords, style, n)
            rows = []
            for cand in candidates:
                hits = ares_search_robust(cand)
                score, match = max_similarity(cand, hits)
                status = "âœ… VolnÃ© (bez blÃ­zkÃ½ch shod)" if score < free_thr else f"âš ï¸ PodobnÃ© (â‰ˆ{score:.0f}% k â€{match}â€œ)"
                rows.append({
                    "NÃ¡vrh": cand,
                    "VÃ½sledek": status,
                    "NejbliÅ¾Å¡Ã­ shoda": match,
                    "SkÃ³re": round(score, 1)
                })
            st.dataframe(rows, use_container_width=True)
