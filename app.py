import os
import re
import requests
import unicodedata
import streamlit as st
from rapidfuzz import fuzz, process
from openai import OpenAI

# ===== API KEY HANDLING =====
def _get_api_key():
    key = os.getenv("OPENAI_API_KEY")
    if key:
        return key
    try:
        return st.secrets["OPENAI_API_KEY"]
    except Exception:
        return None

_api_key = _get_api_key()
if not _api_key:
    st.error("ChybÃ­ OPENAI_API_KEY. Nastav v Railway/Variables nebo ve Streamlit Secrets.")
    st.stop()

client = OpenAI(api_key=_api_key)

# ===== NORMALIZATION =====
LEGAL_SUFFIXES = [
    "s.r.o.", "sro", "a.s.", "as", "k.s.", "ks", "v.o.s.", "vos", "spol. s r.o.", "spol s r o"
]
GENERIC_WORDS = [
    "cz","czech","praha","brno","plzen","ostrava","group","holding","solutions","consulting",
    "system","systems","technology","technologies","services","service","studio","company","co",
    "global","international","advisory","advisers","adviser"
]

def strip_diacritics(s: str) -> str:
    norm = unicodedata.normalize("NFD", s)
    return "".join(ch for ch in norm if unicodedata.category(ch) != "Mn")

def norm_core(name: str) -> str:
    if not name:
        return ""
    s = name.lower().strip()
    s = strip_diacritics(s)
    for suf in LEGAL_SUFFIXES:
        s = s.replace(suf, " ")
    s = re.sub(r"[^0-9a-zA-Z\s]", " ", s)
    s = " ".join(s.split())
    parts = [p for p in s.split() if p not in GENERIC_WORDS]
    return " ".join(parts)

# ===== ARES SEARCH (v2 REST, zdroj OR) =====
def ares_query(obchodni_jmeno: str, count: int = 50):
    base = "https://ares.gov.cz/ekonomicke-subjekty-v2/ekonomicke-subjekty"
    params = {
        "obchodniJmeno": obchodni_jmeno,
        "pocet": str(count),
        "razeni": "obchodniJmeno@asc",
        "zdroj": "OR"
    }
    headers = {
        "Accept": "application/json",
        "User-Agent": "cz-name-checker/1.1 (+contact: owner@example.com)"
    }
    r = requests.get(base, params=params, headers=headers, timeout=20)
    r.raise_for_status()
    data = r.json()
    items = data.get("ekonomickeSubjekty", []) or data.get("vysledky", [])
    out = []
    for it in items:
        of = it.get("obchodniJmeno") or it.get("obchodniJmenoText")
        ico = it.get("ico")
        if of:
            out.append({"name": of, "ico": ico})
    return out

def ares_search_robust(candidate: str, count: int = 50):
    queries = []
    full = candidate.strip()
    queries.append(full)
    queries.append(strip_diacritics(full))
    core = norm_core(candidate)
    core_words = core.split()
    if len(core_words) >= 1:
        queries.append(core_words[0])
    if len(core_words) >= 2:
        queries.append(" ".join(core_words[:2]))
    seen = set()
    hits = []
    for q in queries:
        q = q.strip()
        if not q or q.lower() in seen:
            continue
        seen.add(q.lower())
        try:
            hits += ares_query(q, count)
        except Exception:
            pass
        dedup = {}
        for h in hits:
            key = norm_core(h["name"])
            if key not in dedup:
                dedup[key] = h
        hits = list(dedup.values())
    return hits

# ===== SIMILARITY (RapidFuzz token set) =====
def best_similarity(candidate: str, corpus: list):
    names = [h["name"] for h in corpus]
    if not names:
        return None, 0
    match, score, idx = process.extractOne(candidate, names, scorer=fuzz.token_set_ratio)
    return match, score

# ===== GENERATE NAMES =====
def generate_names(keywords, style, n=10):
    prompt = (
        f"Vymysli {n} originÃ¡lnÃ­ch nÃ¡zvÅ¯ firmy pro: {keywords}. "
        f"Styl: {style or 'modernÃ­, struÄnÃ©, nezamÄ›nitelnÃ©'}. "
        f"NepÅ™idÃ¡vej prÃ¡vnÃ­ pÅ™Ã­pony (s.r.o., a.s.). KaÅ¾dÃ½ nÃ¡zev na novÃ½ Å™Ã¡dek."
    )
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
    )
    text = resp.choices[0].message.content.strip()
    names = [line.strip("-â€¢ ").strip() for line in text.split("\n") if line.strip()]
    seen, clean = set(), []
    for nm in names:
        key = norm_core(nm)
        if key and key not in seen:
            seen.add(key)
            clean.append(nm[:70])
    return clean[:n]

# ===== UI =====
st.set_page_config(page_title="GenerÃ¡tor nÃ¡zvÅ¯ + ARES kontrola", layout="centered")
st.title("ğŸ§­ GenerÃ¡tor nÃ¡zvÅ¯ firem + kontrola v ARES (CZ)")

with st.expander("NastavenÃ­"):
    n = st.slider("PoÄet nÃ¡vrhÅ¯", 5, 30, 10)
    style = st.text_input("Styl (volitelnÃ©)", "modernÃ­, struÄnÃ©, nezamÄ›nitelnÃ©")
    max_ares = st.slider("Kolik vÃ½sledkÅ¯ stÃ¡hnout z ARES", 10, 100, 50, step=10)
    high_thr = st.slider("Hranice 'pravdÄ›podobnÄ› zamÄ›nitelnÃ©' (%)", 80, 100, 92)
    med_thr = st.slider("Hranice 'pozor â€“ podobnÃ©' (%)", 70, 95, 85)

keywords = st.text_input("Zadej obor/klÃ­ÄovÃ¡ slova (napÅ™. Axelrod advisory, AI Å¡kolenÃ­, kavÃ¡rna):")

if st.button("Vygenerovat a zkontrolovat"):
    if not keywords.strip():
        st.warning("Zadej aspoÅˆ obor nebo klÃ­ÄovÃ¡ slova.")
        st.stop()
    with st.spinner("ğŸ”® Generuji nÃ¡vrhy..."):
        candidates = generate_names(keywords, style, n)
    st.success("Hotovo. Kontroluji ARESâ€¦")
    results = []
    for cand in candidates:
        hits = ares_search_robust(cand, count=max_ares)
        match, score = best_similarity(cand, hits)
        if score >= high_thr:
            status = f"âŒ PravdÄ›podobnÄ› zamÄ›nitelnÃ© (â‰ˆ{score:.0f}% k â€{match}â€œ)"
        elif score >= med_thr:
            status = f"ğŸŸ¨ Pozor â€“ podobnÃ© (â‰ˆ{score:.0f}% k â€{match}â€œ)"
        else:
            status = "âœ… VolnÃ© (bez blÃ­zkÃ½ch shod)"
        results.append({"NÃ¡vrh": cand, "VÃ½sledek": status})
    st.subheader("VÃ½sledky")
    st.dataframe(results, use_container_width=True)
    st.caption("Pozn.: Kontrola je heuristickÃ¡. FinÃ¡lnÃ­ posouzenÃ­ dÄ›lÃ¡ rejstÅ™Ã­kovÃ½ soud.")
